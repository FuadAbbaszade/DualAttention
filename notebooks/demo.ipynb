{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual-Attention Whisper Demo\n",
    "\n",
    "This notebook demonstrates how to use the Dual-Attention Whisper model for noise-robust speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import torch\n",
    "from transformers import WhisperProcessor\n",
    "from model.dual_whisper import create_dual_attention_whisper\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_name = \"openai/whisper-small\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = create_dual_attention_whisper(\n",
    "    model_name=model_name,\n",
    "    freeze_encoder=True\n",
    ")\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Play Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your audio file\n",
    "audio_path = \"path/to/your/audio.wav\"\n",
    "\n",
    "audio, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "# Play audio\n",
    "print(f\"Audio duration: {len(audio)/sr:.2f} seconds\")\n",
    "ipd.Audio(audio, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transcribe Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "input_features = processor.feature_extractor(\n",
    "    audio,\n",
    "    sampling_rate=16000,\n",
    "    return_tensors=\"pt\"\n",
    ").input_features\n",
    "\n",
    "# Generate transcription\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_features,\n",
    "        language=\"az\",  # Change to your language\n",
    "        task=\"transcribe\",\n",
    "        max_length=448,\n",
    "        num_beams=5\n",
    "    )\n",
    "\n",
    "# Decode\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"\\nTranscription:\")\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare with Standard Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "# Load standard Whisper\n",
    "standard_model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "standard_model.eval()\n",
    "\n",
    "# Generate with standard Whisper\n",
    "with torch.no_grad():\n",
    "    standard_ids = standard_model.generate(\n",
    "        input_features,\n",
    "        language=\"az\",\n",
    "        task=\"transcribe\",\n",
    "        max_length=448,\n",
    "        num_beams=5\n",
    "    )\n",
    "\n",
    "standard_transcription = processor.batch_decode(standard_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Standard Whisper:\")\n",
    "print(standard_transcription)\n",
    "print(\"\\nDual-Attention Whisper:\")\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import NoisyAudioDataset\n",
    "from data.collator import DataCollatorSpeechSeq2SeqWithPadding\n",
    "from training.metrics import compute_metrics\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = NoisyAudioDataset(\n",
    "    data_path=\"../data/processed/train.json\",\n",
    "    processor=processor,\n",
    "    language=\"az\"\n",
    ")\n",
    "\n",
    "eval_dataset = NoisyAudioDataset(\n",
    "    data_path=\"../data/processed/eval.json\",\n",
    "    processor=processor,\n",
    "    language=\"az\"\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../outputs\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-6,\n",
    "    warmup_steps=500,\n",
    "    max_steps=5000,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics(processor),\n",
    "    processing_class=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Attention Weights (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate with attention output\n",
    "outputs = model.generate(\n",
    "    input_features,\n",
    "    language=\"az\",\n",
    "    task=\"transcribe\",\n",
    "    output_attentions=True,\n",
    "    return_dict_in_generate=True,\n",
    "    max_length=50\n",
    ")\n",
    "\n",
    "# Note: Extracting and visualizing dual attention weights requires\n",
    "# additional code to access the primary vs secondary attention patterns\n",
    "# This is left as an exercise for advanced users\n",
    "\n",
    "print(\"Attention visualization would go here\")\n",
    "print(\"You can access cross-attention weights from the decoder outputs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
